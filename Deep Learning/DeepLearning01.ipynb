{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWOfCg4l9Fdp"
      },
      "outputs": [],
      "source": [
        "#Deep Learning Day 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyTorch core libraries\n",
        "import torch            # Main PyTorch library for tensor operations\n",
        "import torch.nn as nn   # Neural network modules and layers\n",
        "import torch.optim as optim  # Optimization algorithms like SGD, Adam"
      ],
      "metadata": {
        "id": "KOKIhiL19JtA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Perceptron model class inheriting from nn.Module\n",
        "class Perceptron(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Perceptron, self).__init__()\n",
        "        # Define a linear layer with 'input_size' inputs and 1 output\n",
        "        # This layer will learn weights and a bias term\n",
        "        self.linear = nn.Linear(input_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass: input tensor x passes through linear layer\n",
        "        out = self.linear(x)\n",
        "        # Apply sigmoid activation function to squash output between 0 and 1\n",
        "        # Sigmoid outputs probability-like values suitable for binary classification\n",
        "        out = torch.sigmoid(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "jvxKiXoe9UQP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare training data for the AND logic gate\n",
        "# Inputs (X) are all combinations of 0 and 1 for two variables\n",
        "X = torch.tensor([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "], dtype=torch.float32)  # Use float32 for compatibility with PyTorch\n",
        "\n",
        "# Labels (y) are the expected outputs of AND gate:\n",
        "# Only [1,1] maps to 1, all others map to 0\n",
        "y = torch.tensor([\n",
        "    [0],\n",
        "    [0],\n",
        "    [0],\n",
        "    [1]\n",
        "], dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "R7VDy1if9p4a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the Perceptron model with 2 input features\n",
        "model = Perceptron(input_size=2)"
      ],
      "metadata": {
        "id": "qB6sJ7zc9xoQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function:\n",
        "# Binary Cross Entropy Loss measures the difference between predicted probabilities and true labels\n",
        "criterion = nn.BCELoss()\n"
      ],
      "metadata": {
        "id": "1OibDdFa91sV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer:\n",
        "# Stochastic Gradient Descent (SGD) will adjust model weights to minimize loss\n",
        "# Learning rate (lr) controls step size during weight updates\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "IieiWG1s91th"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs (full passes over the dataset)\n",
        "epochs = 1000"
      ],
      "metadata": {
        "id": "OSY78Bg39-rm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()              # Set model to training mode (important for some layers)\n",
        "    optimizer.zero_grad()      # Clear previously computed gradients to avoid accumulation\n",
        "\n",
        "    outputs = model(X)         # Forward pass: predict outputs for all inputs in batch\n",
        "    loss = criterion(outputs, y)  # Calculate how far off predictions are from true labels\n",
        "\n",
        "    loss.backward()            # Backpropagation: compute gradients of loss w.r.t. weights\n",
        "    optimizer.step()           # Update weights using gradients and optimizer algorithm\n",
        "\n",
        "    # Print training loss every 100 epochs to monitor progress\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMjJEWpM9_7v",
        "outputId": "c2f78d86-c54a-4408-a9fb-f2cae64e0435"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6083\n",
            "Epoch 100, Loss: 0.4411\n",
            "Epoch 200, Loss: 0.3500\n",
            "Epoch 300, Loss: 0.2927\n",
            "Epoch 400, Loss: 0.2529\n",
            "Epoch 500, Loss: 0.2231\n",
            "Epoch 600, Loss: 0.1999\n",
            "Epoch 700, Loss: 0.1811\n",
            "Epoch 800, Loss: 0.1656\n",
            "Epoch 900, Loss: 0.1525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training is complete, evaluate the model on training data\n",
        "model.eval()  # Set model to evaluation mode (turns off behaviors like dropout)\n",
        "\n",
        "with torch.no_grad():  # Disable gradient computation since we are only testing\n",
        "    print(\"\\nTesting results:\")\n",
        "    for xi in X:\n",
        "        # Unsqueeze adds batch dimension to input tensor (shape: [1, 2]) because model expects batches\n",
        "        output = model(xi.unsqueeze(0))\n",
        "        # Convert sigmoid output probability to binary prediction using 0.5 threshold\n",
        "        prediction = 1 if output.item() >= 0.5 else 0\n",
        "        # Print the input and predicted output\n",
        "        print(f\"Input: {xi.tolist()} => Predicted output: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8shm7Mq-isc",
        "outputId": "e0914cf0-4e12-4103-a7cc-e7211cdacb04"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing results:\n",
            "Input: [0.0, 0.0] => Predicted output: 0\n",
            "Input: [0.0, 1.0] => Predicted output: 0\n",
            "Input: [1.0, 0.0] => Predicted output: 0\n",
            "Input: [1.0, 1.0] => Predicted output: 1\n"
          ]
        }
      ]
    }
  ]
}